{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RByeKCfdaSZ7"
      },
      "source": [
        "# LELA32051 Computational Linguistics Week 4\n",
        "\n",
        "This week we are going to take a close look at 1-layer neural networks, also known as perceptrons. These were introduced to you in abstract in the lecture and in this seminar we are going to look at how they work in reality.\n",
        "\n",
        "Perceptrons are commonly used as binary classifiers - applying one of two possible labels to input. The example that we are going to look at today is sentiment classification, where we classify a text as having either a \"negative\" or \"positive\" perspective on whatever it is discussing, e.g. a product it is reviewing.\n",
        "\n",
        "Note: the code is heavily based on examples in Chapter 3 of Rao, D., & McMahan, B. (2019). Natural language processing with PyTorch: build intelligent language applications using deep learning. O'Reilly Media, Inc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1U85sASKSul"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/compling23/main/CL_Week_4_Materials/model.pth\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling23/main/CL_Week_4_Materials/nn_tools.py\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling23/main/CL_Week_4_Materials/nn_tools2.py\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling23/main/CL_Week_4_Materials/reviews_with_splits_lite.csv\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling23/main/CL_Week_4_Materials/vectorizer.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpnWOxajoiYY"
      },
      "source": [
        "### Importing modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGcsUizUB7b6"
      },
      "source": [
        "The most important thing we are importing here is PyTorch (https://pytorch.org/). This is one of the two widely used neural network/deep learning packages, the other being TensorFlow (https://www.tensorflow.org/). Both are great! We use PyTorch here because we have to choose and it is slightly more intuitive when first encountered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqFmkcr0oiYY"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from nn_tools import Vocabulary, ReviewVectorizer, ReviewDataset, ReviewClassifier\n",
        "from nn_tools2 import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXOjuJWa9B93"
      },
      "source": [
        "###Organizing code in Python (a very brief intro)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFdxhRcsGqMl"
      },
      "source": [
        "### Functions\n",
        "The first thing you will notice is that we are starting to define our own functions (https://www.w3schools.com/python/python_functions.asp):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPN4KZXEGpjK"
      },
      "outputs": [],
      "source": [
        "def tell_my_name(name):\n",
        "  return(\"My name is \" + name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdU5j_UlHH-X"
      },
      "outputs": [],
      "source": [
        "print(tell_my_name(\"Colin\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_SXW3n39tZh"
      },
      "source": [
        "### Objects\n",
        "\n",
        "A second thing you will see is that we start to define our own objects (https://www.w3schools.com/python/python_classes.asp):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGT7CFpK_uCX"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def tell_my_name(name):\n",
        "        return(\"My name is \" + name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn002UWEArna"
      },
      "outputs": [],
      "source": [
        "print(Agent.tell_my_name(\"Colin\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoNTVeOS_up0"
      },
      "source": [
        "As well as being a way to organize functions this can be a way to store and group variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4QChM-S94p-"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, name=\"\",workplace=\"\"):\n",
        "        self.name=name\n",
        "        self.workplace=workplace\n",
        "    def introduce_myself(self):\n",
        "        return(\"My name is \" + self.name + \" and I work in \" + self.workplace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro3-BIHF-UyU"
      },
      "outputs": [],
      "source": [
        "agent_colin = Agent(\"Colin\",\"Manchester\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR_H9ohf_JRt"
      },
      "outputs": [],
      "source": [
        "print(agent_colin.introduce_myself())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMi_4fF8Zp3N"
      },
      "source": [
        "## Single Layer Networks in PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_PI2ySZBSsJ"
      },
      "source": [
        "### Defining our Single layer network (aka Perceptron)\n",
        "\n",
        "We need to define our object type Perceptron. In doing so we make use of perhaps the most powerful property of objects with is inheritance - we define Perceptron as a subclass of the PyTorch object nn.Module and thereby inherit all of the attributes and functions from that object type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQA_P6NyEyYR"
      },
      "outputs": [],
      "source": [
        "class Perceptron(nn.Module):\n",
        "    \"\"\" A Perceptron is one Linear layer \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): size of the input features\n",
        "        \"\"\"\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x_in):\n",
        "        \"\"\"The forward pass of the MLP\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, input_dim)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, 1)\n",
        "        \"\"\"\n",
        "        return torch.sigmoid(self.fc1(x_in))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yud0AcZ8oiYW"
      },
      "source": [
        "## Classifying Yelp Reviews\n",
        "\n",
        "We have 56000 reviews on Yelp classified as negative (1 or 2 star) or positive (3 or 4 star). We are going to train a classifier using this a part of this data and test its performance on another part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1ek7yp-oiYg"
      },
      "source": [
        "### Settings and some prep work\n",
        "\n",
        "We first load in our data, and set some parameters for use in training. We are also going to import a pretrained model (to save time, although there is code for model training below if you want to try this out)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCssGfrRoiYg"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and Path information\n",
        "    frequency_cutoff=25,\n",
        "    model_state_file='model.pth',\n",
        "    review_csv='reviews_with_splits_lite.csv',\n",
        "    save_dir='.',\n",
        "    vectorizer_file='vectorizer.json',\n",
        "    # No Model hyper parameters\n",
        "    # Training hyper parameters\n",
        "    batch_size=128,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=100,\n",
        "    seed=1337,\n",
        "    # Runtime options\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "\n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXo-xNyzc9ke"
      },
      "outputs": [],
      "source": [
        "reviews = pd.read_csv(args.review_csv)\n",
        "print(reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXCyTwt_oiYi"
      },
      "source": [
        "### Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Y_5EknoiYi"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    print(\"Loading dataset and vectorizer\")\n",
        "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv,\n",
        "                                                            args.vectorizer_file)\n",
        "else:\n",
        "    print(\"Loading dataset and creating vectorizer\")\n",
        "    # create dataset and vectorizer\n",
        "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etKuqCZqEJEn"
      },
      "source": [
        "we are going to use what is known as one-hot coding (in statistics it is called dummy coding) for words. For a vocab of size N we have N dimensions. Each dimension has the value of 1 for a single word and zero for all others. For example, the first 50 dimensions for the word \"can\" look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA8w7He2sPoo"
      },
      "outputs": [],
      "source": [
        "vectorizer.vectorize('can')[0:50]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVfJ3jY0GPMc"
      },
      "source": [
        "Data is then input to the Perceptron in this format and weights learned for each dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIIE7R_yGOD2"
      },
      "outputs": [],
      "source": [
        "classifier.fc1.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJw5KYCSGdn_"
      },
      "outputs": [],
      "source": [
        "classifier.fc1.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtgFE2I1oiYj"
      },
      "source": [
        "### Classifying instances\n",
        "\n",
        "We now define a function predict_rating which will allow us to assign labels to previously unseen reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEZnPllwoiYk"
      },
      "outputs": [],
      "source": [
        "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
        "    \"\"\"Predict the rating of a review\n",
        "\n",
        "    Args:\n",
        "        review (str): the text of the review\n",
        "        classifier (ReviewClassifier): the trained model\n",
        "        vectorizer (ReviewVectorizer): the corresponding vectorizer\n",
        "        decision_threshold (float): The numerical boundary which separates the rating classes\n",
        "    \"\"\"\n",
        "    review = preprocess_text(review)\n",
        "\n",
        "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
        "    result = classifier(vectorized_review.view(1, -1))\n",
        "\n",
        "    probability_value = torch.sigmoid(result).item()\n",
        "    index = 1\n",
        "    if probability_value < decision_threshold:\n",
        "        index = 0\n",
        "\n",
        "    return vectorizer.rating_vocab.lookup_index(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peRbWqSaoiYk"
      },
      "outputs": [],
      "source": [
        "test_review = \"this is a pretty awesome book\"\n",
        "\n",
        "classifier = classifier.cpu()\n",
        "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
        "print(\"{} -> {}\".format(test_review, prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7e1erdWUCHh"
      },
      "outputs": [],
      "source": [
        "test_review = \"this is a pretty terrible book\"\n",
        "\n",
        "classifier = classifier.cpu()\n",
        "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
        "print(\"{} -> {}\".format(test_review, prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbOfq6w3oiYi"
      },
      "source": [
        "### Run on Test Data\n",
        "\n",
        "To evaluate overall performance we can run on our test data and compare the ratings with our annotations in order to calculate an accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t9rPf-QoiYj"
      },
      "outputs": [],
      "source": [
        "# compute the loss & accuracy on the test set using the best available model\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "train_state = make_train_state(args)\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "classifier = classifier.to(args.device)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset,\n",
        "                                   batch_size=args.batch_size,\n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "\n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNpCZfJgoiYj"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nF92vWHoiYk"
      },
      "source": [
        "### Interpretability\n",
        "\n",
        "The simplicity of the Perceptron (it only has 1 layer) means that it is straightforward to interpret by looking at model weights. When models have more layers (or even once we start using representations other than one hot encoding) this becomes much more difficult!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huHrwlgHoiYl"
      },
      "outputs": [],
      "source": [
        "# Sort weights\n",
        "fc1_weights = classifier.fc1.weight.detach()[0]\n",
        "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
        "indices = indices.cpu().numpy().tolist()\n",
        "\n",
        "# Top 20 words\n",
        "print(\"Influential words in Positive Reviews:\")\n",
        "print(\"--------------------------------------\")\n",
        "for i in range(20):\n",
        "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
        "\n",
        "print(\"====\\n\\n\\n\")\n",
        "\n",
        "# Top 20 negative words\n",
        "print(\"Influential words in Negative Reviews:\")\n",
        "print(\"--------------------------------------\")\n",
        "indices.reverse()\n",
        "for i in range(20):\n",
        "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyKVjflaoiYl"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Here, for completeness, is the loop that was used for model training. You can run it, but it will take a while.\n",
        "\n",
        "If you do this, it will run faster if you switch to using a processor type know as a GPU. You can do this as follows:\n",
        "\n",
        "Navigate to Editâ†’Notebook Settings\n",
        "select GPU from the Hardware Accelerator drop-down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PvhWBYooiYi"
      },
      "outputs": [],
      "source": [
        "classifier = classifier.to(args.device)\n",
        "\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                 mode='min', factor=0.5,\n",
        "                                                 patience=1)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "epoch_bar = tqdm_notebook(desc='training routine',\n",
        "                          total=args.num_epochs,\n",
        "                          position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm_notebook(desc='split=train',\n",
        "                          total=dataset.get_num_batches(args.batch_size),\n",
        "                          position=1,\n",
        "                          leave=True)\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm_notebook(desc='split=val',\n",
        "                        total=dataset.get_num_batches(args.batch_size),\n",
        "                        position=1,\n",
        "                        leave=True)\n",
        "\n",
        "try:\n",
        "    for epoch_index in range(args.num_epochs):\n",
        "        train_state['epoch_index'] = epoch_index\n",
        "\n",
        "        # Iterate over training dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "        dataset.set_split('train')\n",
        "        batch_generator = generate_batches(dataset,\n",
        "                                           batch_size=args.batch_size,\n",
        "                                           device=args.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        classifier.train()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # the training routine is these 5 steps:\n",
        "\n",
        "            # --------------------------------------\n",
        "            # step 1. zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # step 2. compute the output\n",
        "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # step 4. use loss to produce gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # step 5. use optimizer to take gradient step\n",
        "            optimizer.step()\n",
        "            # -----------------------------------------\n",
        "            # compute the accuracy\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            # update bar\n",
        "            train_bar.set_postfix(loss=running_loss,\n",
        "                                  acc=running_acc,\n",
        "                                  epoch=epoch_index)\n",
        "            train_bar.update()\n",
        "\n",
        "        train_state['train_loss'].append(running_loss)\n",
        "        train_state['train_acc'].append(running_acc)\n",
        "\n",
        "        # Iterate over val dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "        dataset.set_split('val')\n",
        "        batch_generator = generate_batches(dataset,\n",
        "                                           batch_size=args.batch_size,\n",
        "                                           device=args.device)\n",
        "        running_loss = 0.\n",
        "        running_acc = 0.\n",
        "        classifier.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "            # compute the output\n",
        "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            val_bar.set_postfix(loss=running_loss,\n",
        "                                acc=running_acc,\n",
        "                                epoch=epoch_index)\n",
        "            val_bar.update()\n",
        "\n",
        "        train_state['val_loss'].append(running_loss)\n",
        "        train_state['val_acc'].append(running_acc)\n",
        "\n",
        "        train_state = update_train_state(args=args, model=classifier,\n",
        "                                         train_state=train_state)\n",
        "\n",
        "        scheduler.step(train_state['val_loss'][-1])\n",
        "\n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "\n",
        "        if train_state['stop_early']:\n",
        "            break\n",
        "\n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Exiting loop\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "156px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": "5",
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}