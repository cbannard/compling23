{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag1DD6P8qBVx"
      },
      "source": [
        "# LELA70331 Computational Linguistics Week 8\n",
        "\n",
        "This week we are going to take a look at text generation and dialogue systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJZuIoTdVRQ3"
      },
      "source": [
        "### Text Generation\n",
        "\n",
        "We are going to look first at methods for creative generation of text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6eoVMreVsIk"
      },
      "source": [
        "### Ngram-based generation\n",
        "\n",
        "The simplest way to perform generation is to learn and apply n-gram probabilities. The way this works is that we first calculate the probability of each word in a corpus following each other word. We then pick a random word to start the sentence and the select a next word that is probable given that word. We then output that next word and then select another word that is probable given THAT word. We then repeat for N words.\n",
        "\n",
        "This allows us to generate sentences that look a little like natural language, and to imitate particular genres and styles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO-oFKM8B8PJ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.corpus.gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMS8pC3HVWNZ"
      },
      "outputs": [],
      "source": [
        "carroll = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
        "kjv = nltk.corpus.gutenberg.words('bible-kjv.txt')\n",
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
        "bigrams = nltk.bigrams(kjv)\n",
        "cfd = nltk.ConditionalFreqDist(bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ1SrMTFNs82"
      },
      "outputs": [],
      "source": [
        "cfd['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d19v--fP_4U"
      },
      "outputs": [],
      "source": [
        "def generate_argmax(cfdist, word, num=15):\n",
        "    for i in range(num):\n",
        "        print(word, end=' ')\n",
        "        word = cfdist[word].max()\n",
        "import random\n",
        "def generate_random(cfdist, word, num=15):\n",
        "    for i in range(num):\n",
        "        print(word, end=' ')\n",
        "        word = random.choices(list(cfd[word].keys()), weights=cfd[word].values(), k=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0fs4DUSUsdy"
      },
      "outputs": [],
      "source": [
        "generate_argmax(cfd,\"the\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KY1KS0HTFgH"
      },
      "outputs": [],
      "source": [
        "generate_random(cfd,\"the\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjH9mD5rHnhe"
      },
      "source": [
        "### Unconditioned Generation with Recurrent Neural Networks\n",
        "\n",
        "This actually works very similarly in that it randomly picks each element of the sequence we are generating based on its probability in context. The difference is that the probabilities are based on the softmax output layer in a neural network rather than bigrams.\n",
        "\n",
        "We are going to randomly generate surnames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faDP0p4bHs3B"
      },
      "outputs": [],
      "source": [
        "from generation_tools import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTrTpUZnHxLU"
      },
      "outputs": [],
      "source": [
        "class SurnameGenerationModel(nn.Module):\n",
        "    def __init__(self, char_embedding_size, char_vocab_size, rnn_hidden_size,\n",
        "                 batch_first=True, padding_idx=0, dropout_p=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            char_embedding_size (int): The size of the character embeddings\n",
        "            char_vocab_size (int): The number of characters to embed\n",
        "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
        "            batch_first (bool): Informs whether the input tensors will\n",
        "                have batch or the sequence on the 0th dimension\n",
        "            padding_idx (int): The index for the tensor padding;\n",
        "                see torch.nn.Embedding\n",
        "            dropout_p (float): the probability of zeroing activations using\n",
        "                the dropout method.  higher means more likely to zero.\n",
        "        \"\"\"\n",
        "        super(SurnameGenerationModel, self).__init__()\n",
        "\n",
        "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
        "                                     embedding_dim=char_embedding_size,\n",
        "                                     padding_idx=padding_idx)\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=char_embedding_size,\n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=batch_first)\n",
        "\n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
        "                            out_features=char_vocab_size)\n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the model\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, char_vocab_size)\n",
        "        \"\"\"\n",
        "        x_embedded = self.char_emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        batch_size, seq_size, feat_size = y_out.shape\n",
        "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
        "\n",
        "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        new_feat_size = y_out.shape[-1]\n",
        "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
        "\n",
        "        return y_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7andkb_H3yE"
      },
      "outputs": [],
      "source": [
        "def sample_from_model(model, vectorizer, num_samples=1, sample_size=20,\n",
        "                      temperature=1.0):\n",
        "    \"\"\"Sample a sequence of indices from the model\n",
        "\n",
        "    Args:\n",
        "        model (SurnameGenerationModel): the trained model\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
        "        num_samples (int): the number of samples\n",
        "        sample_size (int): the max length of the samples\n",
        "        temperature (float): accentuates or flattens\n",
        "            the distribution.\n",
        "            0.0 < temperature < 1.0 will make it peakier.\n",
        "            temperature > 1.0 will make it more uniform\n",
        "    Returns:\n",
        "        indices (torch.Tensor): the matrix of indices;\n",
        "        shape = (num_samples, sample_size)\n",
        "    \"\"\"\n",
        "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index\n",
        "                       for _ in range(num_samples)]\n",
        "    begin_seq_index = torch.tensor(begin_seq_index,\n",
        "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
        "    indices = [begin_seq_index]\n",
        "    h_t = None\n",
        "\n",
        "    for time_step in range(sample_size):\n",
        "        x_t = indices[time_step]\n",
        "        x_emb_t = model.char_emb(x_t)\n",
        "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
        "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
        "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
        "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
        "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
        "    return indices\n",
        "\n",
        "def decode_samples(sampled_indices, vectorizer):\n",
        "    \"\"\"Transform indices into the string form of a surname\n",
        "\n",
        "    Args:\n",
        "        sampled_indices (torch.Tensor): the inidces from `sample_from_model`\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
        "    \"\"\"\n",
        "    decoded_surnames = []\n",
        "    vocab = vectorizer.char_vocab\n",
        "\n",
        "    for sample_index in range(sampled_indices.shape[0]):\n",
        "        surname = \"\"\n",
        "        for time_step in range(sampled_indices.shape[1]):\n",
        "            sample_item = sampled_indices[sample_index, time_step].item()\n",
        "            if sample_item == vocab.begin_seq_index:\n",
        "                continue\n",
        "            elif sample_item == vocab.end_seq_index:\n",
        "                break\n",
        "            else:\n",
        "                surname += vocab.lookup_index(sample_item)\n",
        "        decoded_surnames.append(surname)\n",
        "    return decoded_surnames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cirv00CmIFcp"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and Path information\n",
        "    surname_csv=\"surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"ugen_vectorizer.json\",\n",
        "    model_state_file=\"ugen_model.pth\",\n",
        "    save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    # Model hyper parameters\n",
        "    char_embedding_size=32,\n",
        "    rnn_hidden_size=32,\n",
        "    # Training hyper parameters\n",
        "    seed=1337,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=128,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    # Runtime options\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "\n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txMECyJUIGVq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "model = SurnameGenerationModel(char_embedding_size=args.char_embedding_size,\n",
        "                               char_vocab_size=len(vectorizer.char_vocab),\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "model.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "model = model.to(args.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsPcEUe5IKfI"
      },
      "outputs": [],
      "source": [
        "# number of names to generate\n",
        "num_names = 10\n",
        "model = model.cpu()\n",
        "# Generate nationality hidden state\n",
        "sampled_surnames = decode_samples(\n",
        "    sample_from_model(model, vectorizer, num_samples=num_names),\n",
        "    vectorizer)\n",
        "# Show results\n",
        "print (\"-\"*15)\n",
        "for i in range(num_names):\n",
        "    print (sampled_surnames[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8zGHzgRPxXS"
      },
      "source": [
        "### Conditioned Generation\n",
        "\n",
        "We now repeat this, but we condition the output of the network on something that isn't in the sequence - a target nationality for the surname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNNm6G-tP0jq"
      },
      "outputs": [],
      "source": [
        "class SurnameGenerationModel(nn.Module):\n",
        "    def __init__(self, char_embedding_size, char_vocab_size, num_nationalities,\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0, dropout_p=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            char_embedding_size (int): The size of the character embeddings\n",
        "            char_vocab_size (int): The number of characters to embed\n",
        "            num_nationalities (int): The size of the prediction vector\n",
        "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
        "            batch_first (bool): Informs whether the input tensors will\n",
        "                have batch or the sequence on the 0th dimension\n",
        "            padding_idx (int): The index for the tensor padding;\n",
        "                see torch.nn.Embedding\n",
        "            dropout_p (float): the probability of zeroing activations using\n",
        "                the dropout method.  higher means more likely to zero.\n",
        "        \"\"\"\n",
        "        super(SurnameGenerationModel, self).__init__()\n",
        "\n",
        "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
        "                                     embedding_dim=char_embedding_size,\n",
        "                                     padding_idx=padding_idx)\n",
        "\n",
        "        self.nation_emb = nn.Embedding(num_embeddings=num_nationalities,\n",
        "                                       embedding_dim=rnn_hidden_size)\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=char_embedding_size,\n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=batch_first)\n",
        "\n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
        "                            out_features=char_vocab_size)\n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "\n",
        "    def forward(self, x_in, nationality_index, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the model\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, max_seq_size)\n",
        "            nationality_index (torch.Tensor): The index of the nationality for each data point\n",
        "                Used to initialize the hidden state of the RNN\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, char_vocab_size)\n",
        "        \"\"\"\n",
        "        x_embedded = self.char_emb(x_in)\n",
        "\n",
        "        # hidden_size: (num_layers * num_directions, batch_size, rnn_hidden_size)\n",
        "        nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded, nationality_embedded)\n",
        "\n",
        "        batch_size, seq_size, feat_size = y_out.shape\n",
        "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
        "\n",
        "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        new_feat_size = y_out.shape[-1]\n",
        "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
        "\n",
        "        return y_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRxBaKnoQc6x"
      },
      "outputs": [],
      "source": [
        "def sample_from_model(model, vectorizer, nationalities, sample_size=20,\n",
        "                      temperature=1.0):\n",
        "    \"\"\"Sample a sequence of indices from the model\n",
        "\n",
        "    Args:\n",
        "        model (SurnameGenerationModel): the trained model\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
        "        nationalities (list): a list of integers representing nationalities\n",
        "        sample_size (int): the max length of the samples\n",
        "        temperature (float): accentuates or flattens\n",
        "            the distribution.\n",
        "            0.0 < temperature < 1.0 will make it peakier.\n",
        "            temperature > 1.0 will make it more uniform\n",
        "    Returns:\n",
        "        indices (torch.Tensor): the matrix of indices;\n",
        "        shape = (num_samples, sample_size)\n",
        "    \"\"\"\n",
        "    num_samples = len(nationalities)\n",
        "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index\n",
        "                       for _ in range(num_samples)]\n",
        "    begin_seq_index = torch.tensor(begin_seq_index,\n",
        "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
        "    indices = [begin_seq_index]\n",
        "    nationality_indices = torch.tensor(nationalities, dtype=torch.int64).unsqueeze(dim=0)\n",
        "    h_t = model.nation_emb(nationality_indices)\n",
        "\n",
        "    for time_step in range(sample_size):\n",
        "        x_t = indices[time_step]\n",
        "        x_emb_t = model.char_emb(x_t)\n",
        "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
        "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
        "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
        "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
        "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
        "    return indices\n",
        "\n",
        "def decode_samples(sampled_indices, vectorizer):\n",
        "    \"\"\"Transform indices into the string form of a surname\n",
        "\n",
        "    Args:\n",
        "        sampled_indices (torch.Tensor): the inidces from `sample_from_model`\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
        "    \"\"\"\n",
        "    decoded_surnames = []\n",
        "    vocab = vectorizer.char_vocab\n",
        "\n",
        "    for sample_index in range(sampled_indices.shape[0]):\n",
        "        surname = \"\"\n",
        "        for time_step in range(sampled_indices.shape[1]):\n",
        "            sample_item = sampled_indices[sample_index, time_step].item()\n",
        "            if sample_item == vocab.begin_seq_index:\n",
        "                continue\n",
        "            elif sample_item == vocab.end_seq_index:\n",
        "                break\n",
        "            else:\n",
        "                surname += vocab.lookup_index(sample_item)\n",
        "        decoded_surnames.append(surname)\n",
        "    return decoded_surnames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhMRf2byTDdY"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and Path information\n",
        "    surname_csv=\"surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"cgen_vectorizer.json\",\n",
        "    model_state_file=\"cgen_model.pth\",\n",
        "    save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    # Model hyper parameters\n",
        "    char_embedding_size=32,\n",
        "    rnn_hidden_size=32,\n",
        "    # Training hyper parameters\n",
        "    seed=1337,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=128,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    # Runtime options\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "\n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-wcWeg0S8A3"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "model = SurnameGenerationModel(char_embedding_size=args.char_embedding_size,\n",
        "                               char_vocab_size=len(vectorizer.char_vocab),\n",
        "                               num_nationalities=len(vectorizer.nationality_vocab),\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index,\n",
        "                               dropout_p=0.5)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "model.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "model = model.to(args.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjMXT0FITVN7"
      },
      "outputs": [],
      "source": [
        "model = model.cpu()\n",
        "for index in range(len(vectorizer.nationality_vocab)):\n",
        "    nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
        "    print(\"Sampled for {}: \".format(nationality))\n",
        "    sampled_indices = sample_from_model(model, vectorizer,\n",
        "                                        nationalities=[index] * 3,\n",
        "                                        temperature=0.7)\n",
        "    for sampled_surname in decode_samples(sampled_indices, vectorizer):\n",
        "        print(\"-  \" + sampled_surname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbUHS3OhM8zo"
      },
      "source": [
        "### GPT-2\n",
        "\n",
        "Before moving on I just want to bridge the gap between what we have been doing and the kinds of technology you see in real world systems. A number of earlier GPT models (the language model behind ChatGPT) can be downloaded and run on your own machine. We are going to use GPT-2 (https://openai.com/blog/better-language-models/). The basic principle by which generation works is the same as we have seen above - it just uses more data and a slightly different model type.\n",
        "\n",
        "It is also very similar to GPT-4. Except these later models were trained on more data and have many more parameters (cf \"weights\").\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv_QvvOOM_1P"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQwxIp5MNDny"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, set_seed\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjs7aJPHND2s"
      },
      "outputs": [],
      "source": [
        "model_name = 'gpt2-medium'\n",
        "num_samples = 1\n",
        "max_length = 100\n",
        "top_k = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2wFl10HNf1a"
      },
      "outputs": [],
      "source": [
        "#load the model\n",
        "set_seed(42)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlsArg9tNklx"
      },
      "outputs": [],
      "source": [
        "prompts = []\n",
        "prompt = input()\n",
        "prompts.append(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE6HSM-6Npe7"
      },
      "outputs": [],
      "source": [
        "output = {}\n",
        "for text in prompts:\n",
        "\tinput_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\tmax_length = len(text.split()) + max_length\n",
        "\tresponses = []\n",
        "\tresponses = model.generate(input_ids, max_length = max_length, do_sample=True, top_k=top_k, num_return_sequences=num_samples)\n",
        "\n",
        "\tresponses = responses[:, input_ids.shape[-1]:]\n",
        "\toutput[text] = []\n",
        "\tfor i, r in enumerate(responses):\n",
        "\t\tresponse = tokenizer.decode(r, skip_special_tokens=True)\n",
        "\t\toutput[text].append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk5H16DgNth8"
      },
      "outputs": [],
      "source": [
        "for k in output:\n",
        "    print('prompt: ' + k + '\\n')\n",
        "    for i, r in enumerate(output[k]):\n",
        "        print('================================== Output ==================================')\n",
        "        print(k + \" \" + r.strip() + '\\n')\n",
        "    print('********************************************************************************************************************')\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPbNBNRU3soq"
      },
      "source": [
        "## Rule-based chatbot: Eliza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqApcwI73sor"
      },
      "source": [
        "As described in the week 7 lecture, Eliza is a chatbot that simulates a Rogerian therapist, making use of a set of rules in the form of regular expressions. At the heart of Eliza are the substitution function (re.sub in Python) and grouping. We'll start with a quick recap as to what these are.\n",
        "\n",
        "We need to import the Regular Expressions module in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s93lrLpm3sor"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk1wG59t3sos"
      },
      "source": [
        "This gives as access to the very useful function re.sub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "0otwKAex3sos"
      },
      "source": [
        "### re.sub()\n",
        "\n",
        "This finds all occurences of a given sequence and replaces it with a sequence provided:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq4gmf-u3sot"
      },
      "outputs": [],
      "source": [
        "utt = 'walked'\n",
        "re.sub('ed','ing',utt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXxTxkt53sot"
      },
      "source": [
        "### Groups\n",
        "\n",
        "Grouping is a very powerful technique for picking out substrings from a string that matches a specified pattern. Parentheses are used to indicate the start and end of the substring. It is very powerful when combined with substitution.\n",
        "\n",
        "You can use parentheses to capture a particular substring within a pattern and then use it in your replacement string within sub. For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "d8d-MXoZ3sou"
      },
      "outputs": [],
      "source": [
        "utt = \"procrastinating\"\n",
        "re.sub('([a-z]+)ing','\\\\1ed',utt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxHCnYk43sov"
      },
      "source": [
        "## A very simple Elizabot\n",
        "\n",
        "The code below implements a very simple Eliza. The function respond takes an utterance as input and using re.sub to generate responses. The loop below the function creates a simple interface that takes user input and prints the response.\n",
        "\n",
        "We can extend Eliza's ability by adding additional rules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFv8yrpylNuU"
      },
      "outputs": [],
      "source": [
        "def respond(utt):\n",
        "  utt = re.sub('hello my name is (.+)','hello \\\\1 how are you feeling today', utt)\n",
        "  return utt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcIjRBRKlKfi"
      },
      "outputs": [],
      "source": [
        "utt = \"\"\n",
        "while utt != 'goodbye':\n",
        "    utt = input('> ')\n",
        "    reply = respond(utt)\n",
        "    if reply != utt:\n",
        "        print(reply)\n",
        "    else:\n",
        "        if utt != \"goodbye\":\n",
        "            print(\"Can you rephrase that?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUEniCI2qK-X"
      },
      "source": [
        "### Activity\n",
        "\n",
        "Add patterns (using substitutions and grouping) to the respond function that will allow Eliza to conduct both of these conversations. Test your system by conducting the conversation with Eliza.\n",
        "\n",
        "User: hello my name is emma <br>\n",
        "Eliza: Hello emma my name is Eliza. How are you feeling today? <br>\n",
        "User: i am feeling very happy <br>\n",
        "Eliza: Do you often feel happy? <br>\n",
        "User: yes since I started my new job <br>\n",
        "Eliza: Can you tell me about starting your new job? <br>\n",
        "\n",
        "User: hello my name is john <br>\n",
        "Eliza: Hello john, my name is Eliza. How are you feeling today? <br>\n",
        "User: i am feeling pretty happy <br>\n",
        "Eliza: Do you often feel happy? <br>\n",
        "User: yes since I moved house <br>\n",
        "Eliza: Can you tell me about moving house? <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6DF6FOh3sow"
      },
      "source": [
        "### Reverse engineering the NLTK Eliza\n",
        "\n",
        "There have been many implementations of Eliza over the years. One version is built into the NLTK toolkit. This can be run as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1fqK44ethJ3"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yOeuTVotfQJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "nltk.chat.eliza.demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nh04AqY3sox"
      },
      "source": [
        "Activity: Conduct a four-turn-each conversation of your own with the NLTK Eliza. Adds the substitution that you think Eliza is using to generate the responses to your own chatbot using the code below. Where you find Eliza's response to be lacking, update the substitution to give a better response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "8ztFBkZj3soy"
      },
      "outputs": [],
      "source": [
        "def respond(utt):\n",
        "  utt = re.sub('hello my name is (.+)','hello \\\\1 how are you feeling today', utt)\n",
        "  return utt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ftc0J9In3soy"
      },
      "outputs": [],
      "source": [
        "utt = \"\"\n",
        "while utt != 'goodbye':\n",
        "    utt = input('> ')\n",
        "    reply = respond(utt)\n",
        "    if reply != utt:\n",
        "        print(reply)\n",
        "    else:\n",
        "        if utt != \"goodbye\":\n",
        "            print(\"Can you rephrase that?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIDS4M25uKXP"
      },
      "source": [
        "## Corpus-based chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIUnZjlxsUj1"
      },
      "source": [
        "Training and running a corpus-based chatbot takes more steps than we have time for today. If you want to have a go in your own time, then you will find a tutorial for doing so in Pytorch here:\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n",
        "\n",
        "There is a link at the top of the page to open the notebook in Colab.\n",
        "\n",
        "To run this in colab easily just put the following code block at the top and run it before working through the rest of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg6znOWd3so1"
      },
      "outputs": [],
      "source": [
        "! wget https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip\n",
        "! unzip movie-corpus.zip\n",
        "! mkdir data\n",
        "! mv movie-corpus data/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity: Intent classification\n",
        "This is an activity to help you think about the rule-based intent classification component of your coursework. You should write rules to uniquely and correctly identify each of the following utterances:"
      ],
      "metadata": {
        "id": "2WSo1COJHO-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PlayMusic: play the weather girls\n",
        "\n",
        "AddToPlaylist: add this to my italian film soundtrack playlist\n",
        "\n",
        "RateBook: give the restaurant guidebook 5 stars\n",
        "\n",
        "SearchScreeningEvent: find screenings of the book thief at around 7\n",
        "\n",
        "BookRestaurant: book me a table outside for 2 for dinner at the national theatre restaurant\n",
        "\n",
        "GetWeather: will it be warm enough to eat dinner outside at around 7 tonight\n",
        "\n",
        "SearchCreativeWork: find me songs films or books about restaurants\n",
        "\n",
        "Here is the function from your coursework notebook:"
      ],
      "metadata": {
        "id": "Dfwl8dEmHmfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def assign_intent(utt, verbose=False):\n",
        "  PlayMusic_Pattern = re.compile(\"play|music\")\n",
        "  AddToPlaylist_Pattern = re.compile(\"add|playlist\")\n",
        "  RateBook_Pattern = re.compile(\"rate|book\")\n",
        "  SearchScreeningEvent_Pattern = re.compile(\"screening\")\n",
        "  BookRestaurant_Pattern = re.compile(\"book|restaurant|food\")\n",
        "  GetWeather_Pattern = re.compile(\"get|weather\")\n",
        "  SearchCreativeWork_Pattern = re.compile(\"creative\")\n",
        "\n",
        "  weights = {}\n",
        "  weights['PlayMusic'] = len(re.findall(PlayMusic_Pattern,  utt))\n",
        "  weights['AddToPlaylist'] = len(re.findall(AddToPlaylist_Pattern,  utt))\n",
        "  weights['RateBook'] = len(re.findall(RateBook_Pattern,  utt))\n",
        "  weights['SearchScreeningEvent'] = len(re.findall(SearchScreeningEvent_Pattern,  utt))\n",
        "  weights['BookRestaurant'] = len(re.findall(BookRestaurant_Pattern,  utt))\n",
        "  weights['GetWeather'] = len(re.findall(GetWeather_Pattern,  utt))\n",
        "  weights['SearchCreativeWork'] = len(re.findall(SearchCreativeWork_Pattern,  utt))\n",
        "  if verbose:\n",
        "      print(weights)\n",
        "  if max(weights.values()) == 0:\n",
        "      return random.choice(list(weights.keys()))\n",
        "  else:\n",
        "      weights_as_list = list(weights.items())\n",
        "      random.shuffle(weights_as_list)\n",
        "      weights=dict(weights_as_list)\n",
        "      return max(weights, key=lambda key: weights[key])"
      ],
      "metadata": {
        "id": "N9RtO7-2Ht-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_inputs = ['play the weather girls','add this to my italian film soundtrack playlist','give the restaurant guidebook 5 stars','find screenings of the book thief at around 7','book me a table outside for 2 for dinner at the national theatre restaurant','will it be warm enough to eat dinner outside at around 7 tonight','find me songs films or books about restaurants']\n",
        "[print(str(assign_intent(utt)) + \" : \" + utt) for utt in example_inputs]"
      ],
      "metadata": {
        "id": "QwEtGrEWHs10"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Week_7_Seminar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}