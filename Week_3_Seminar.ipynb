{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ada7675f",
      "metadata": {
        "id": "ada7675f"
      },
      "source": [
        "# LELA32051 Computational Linguistics Week 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede3dfeb",
      "metadata": {
        "id": "ede3dfeb"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f508cffb",
      "metadata": {
        "id": "f508cffb"
      },
      "source": [
        "### Escaping special characters\n",
        "We have learned about a number of character that have a special meaning in regular expressions (periods, dollar signs etc). We might sometimes want to search for these characters in strings. To do this we can \"escape\" the character using a backslash() as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02871a24",
      "metadata": {
        "id": "02871a24"
      },
      "outputs": [],
      "source": [
        "opening_sentence = \"On an exceptionally hot evening early in July a young man came out of the garret in which he lodged in S. Place and walked slowly, as though in hesitation, towards K. bridge.\"\n",
        "re.findall(\"\\.\",opening_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f53a7a48",
      "metadata": {
        "id": "f53a7a48"
      },
      "source": [
        "### re.split()\n",
        "In week 1 we learned to tokenise a string using the string function split. re also has a split function. re.split() takes a regular expression as a first argument (unless you have a precompiled pattern) and a string as second argument, and split the string into tokens divided by all substrings matched by the regular expression.\n",
        "Can you improve on the following tokeniser? In doing so you might need to extend your knowledge of regular expressions and employ one of the special characters included here: https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5543015b",
      "metadata": {
        "id": "5543015b"
      },
      "outputs": [],
      "source": [
        "to_split_on_word = re.compile(\" \")\n",
        "opening_sentence_new = to_split_on_word.split(opening_sentence)\n",
        "print(opening_sentence_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cb5bc5b",
      "metadata": {
        "id": "9cb5bc5b"
      },
      "source": [
        "# Sentence Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48df58a8",
      "metadata": {
        "id": "48df58a8"
      },
      "source": [
        "Above we split a sentence into words. However most texts that we want to process have more than one sentence, so we also need to segment text into sentences. We will work with the first chapter of Crime and Punishment again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1dd2b7",
      "metadata": {
        "id": "4a1dd2b7"
      },
      "outputs": [],
      "source": [
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "f = open('2554-0.txt')\n",
        "raw = f.read()\n",
        "chapter_one = raw[5464:23725]\n",
        "chapter_one = re.sub('\\n',' ',chapter_one)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0f08861",
      "metadata": {
        "id": "b0f08861"
      },
      "source": [
        "Just as for segmenting sentences into words, we can segment texts into sentence using the re.split function. If you run the code below you will get a list of words. What pattern could we use to get a list of sentences? Clue: you might want to use an re.sub statement to transform the input before splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da8d3567",
      "metadata": {
        "id": "da8d3567"
      },
      "outputs": [],
      "source": [
        "to_split_on_sent = re.compile(\"\\.\")\n",
        "C_and_P_sentences = to_split_on_sent.split(chapter_one)\n",
        "print(C_and_P_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf5345f",
      "metadata": {
        "id": "cbf5345f"
      },
      "source": [
        "If we combine sentence segmentation and tokenising we can split an input text into a list of sentences, each of which is itself a list of words. But to do this we will need to learn about another important aspect of Python - the \"for loop\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91482388",
      "metadata": {
        "id": "91482388"
      },
      "source": [
        "# Iterating/for loops\n",
        "\n",
        "Humans reading texts do so one word and one sentence at a time. The same is often true for computers. This is most commonly performed using a \"for loop\". This can be straightforwardly implemented for lists. In the following code we iterate through the list printing each entry as we go. Note that the end=\"\" in the print statement tells it to end each printed token with a space rather than a new line which is the default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbdf0364",
      "metadata": {
        "id": "fbdf0364"
      },
      "outputs": [],
      "source": [
        "for word in opening_sentence_new:\n",
        "    print(word, end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89df10d5",
      "metadata": {
        "id": "89df10d5"
      },
      "source": [
        "You will notice that in the loop above the print statement is indented. We say that a statement that occurs within a loop is nested within that loop. Any statement that is nested inside another has to be indented in Python. The standard way to indent is to use 4 spaces, although you can also use a tab.\n",
        "\n",
        "Advanced but worth trying anyway activity:\n",
        "Put this all together so that we segment the sentences and then loop through tokenising each one in turn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8ec3d2",
      "metadata": {
        "id": "3f8ec3d2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Natural Language Toolkit"
      ],
      "metadata": {
        "id": "SrlRLk386H3C"
      },
      "id": "SrlRLk386H3C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we have looked at the core Python programming language and the re library. However much of the time this semester we will be making use of even more  powerful libraries for natural language processing and machine learning. Today we will make use of a few of these. The first of is \"Natural Language Toolkit\" or nltk (http://www.nltk.org/).\n",
        "\n",
        "The first thing we need to do is to make sure we have the libraries we want installed. On Google Colab they are all already there. If your are using your own machine you will have to install it using the following command (unlike for re which is present by default and just needs to be loaded).\n"
      ],
      "metadata": {
        "id": "0_7dXjcU6NPS"
      },
      "id": "0_7dXjcU6NPS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use the library we then need to import it"
      ],
      "metadata": {
        "id": "XeveyQ1U6ZAb"
      },
      "id": "XeveyQ1U6ZAb"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "1OjPJnkF6b_j"
      },
      "id": "1OjPJnkF6b_j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenising"
      ],
      "metadata": {
        "id": "kaDajLqB6fWJ"
      },
      "id": "kaDajLqB6fWJ"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "chapter_one_tokens = nltk.word_tokenize(chapter_one)"
      ],
      "metadata": {
        "id": "S7ENRnXE6r28"
      },
      "id": "S7ENRnXE6r28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Segmentation"
      ],
      "metadata": {
        "id": "ohWHhhsL6386"
      },
      "id": "ohWHhhsL6386"
    },
    {
      "cell_type": "code",
      "source": [
        "chapter_one_sentences = nltk.sent_tokenize(' '.join(chapter_one_tokens))\n",
        "print(chapter_one_sentences[1])"
      ],
      "metadata": {
        "id": "xIBnBecI67FI"
      },
      "id": "xIBnBecI67FI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "PUAqbWbK6_af"
      },
      "id": "PUAqbWbK6_af"
    },
    {
      "cell_type": "code",
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "for t in chapter_one_tokens:\n",
        "    print(porter.stem(t),end=\" \")"
      ],
      "metadata": {
        "id": "1-x4wwnn6_3I"
      },
      "id": "1-x4wwnn6_3I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatising"
      ],
      "metadata": {
        "id": "ApCEr2dc7D47"
      },
      "id": "ApCEr2dc7D47"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "for t in chapter_one_tokens:\n",
        "    print(wnl.lemmatize(t),end=\" \")"
      ],
      "metadata": {
        "id": "HB6ESeQZ7EaR"
      },
      "id": "HB6ESeQZ7EaR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector semantics"
      ],
      "metadata": {
        "id": "l1ijQKCx7HwX"
      },
      "id": "l1ijQKCx7HwX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE FOLLOWING CELL IS TO BE RUN IN THE BREAK. DO NOT RUN BEFORE!"
      ],
      "metadata": {
        "id": "4pfL_g3LWXTH"
      },
      "id": "4pfL_g3LWXTH"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy\n",
        "!pip install torch torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import output\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "1YKzFZgcNdxr"
      },
      "id": "1YKzFZgcNdxr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this week's lecture you heard about Vector-based semantics. Today we will take a look at these models in Python.\n",
        "\n",
        "First we will use nltk to segment and tokenize the whole of Crime and Punishment."
      ],
      "metadata": {
        "id": "5BsrFJSs7IXp"
      },
      "id": "5BsrFJSs7IXp"
    },
    {
      "cell_type": "code",
      "source": [
        "C_and_P_tokens_sentences = []\n",
        "for sent in nltk.sent_tokenize(raw):\n",
        "    C_and_P_tokens_sentences.append(nltk.word_tokenize(sent))"
      ],
      "metadata": {
        "id": "VKgZMdQ_7UbA"
      },
      "id": "VKgZMdQ_7UbA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will build a cooccurence matrix using the following function. The purpose of this is to aid your conceptual understanding by looking at the output, and you aren't expected to read or understand this code."
      ],
      "metadata": {
        "id": "QRf6Fbn97WZl"
      },
      "id": "QRf6Fbn97WZl"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function from https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling\n",
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "\n",
        "    # Get a sorted list of all vocab items\n",
        "    distinct_words = sorted(list(set([word for sentence in corpus for word in sentence])))\n",
        "    # Find vocabulary size\n",
        "    num_words = len(distinct_words)\n",
        "    # Create a Word Dictionary mapping each word to a unique index\n",
        "    word2Ind = {word: index for index, word in enumerate(distinct_words)}\n",
        "\n",
        "    # Create a numpy matrix in order to store co-occurence counts\n",
        "    M = np.zeros((num_words, num_words))\n",
        "\n",
        "    # Iterate over sentences in text\n",
        "    for sentence in corpus:\n",
        "        # Iterate over words in each sentence\n",
        "        for i, word in enumerate(sentence):\n",
        "            # Find the index in the tokenized sentence vector for the beginning of the window (the current token minus window size or zero whichever is the lower)\n",
        "            begin = max(i - window_size, 0)\n",
        "            # Find the index in the tokenized sentence vector for the end of the window (the current token plus window size or the length of the sentence whichever is the lower)\n",
        "            end   = min(i + window_size, num_words)\n",
        "            # Extract the text from beginning of window to the end\n",
        "            context = sentence[begin: end + 1]\n",
        "            # Remove the target word from its own window\n",
        "            context.remove(sentence[i])\n",
        "            # Find the row for the current target word\n",
        "            current_row = word2Ind[word]\n",
        "            # Iterate over the window for this target word\n",
        "            for token in context:\n",
        "                # Find the ID and hence the column index for the current token\n",
        "                current_col = word2Ind[token]\n",
        "                # Add 1 to the current context word dimension for the current target word\n",
        "                M[current_row, current_col] += 1\n",
        "    # Return the co-occurence matrix and the vocabulary to index \"dictionary\"\n",
        "    return M, word2Ind"
      ],
      "metadata": {
        "id": "gO7kKXbi7e5V"
      },
      "id": "gO7kKXbi7e5V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function allows us to specify the window that we use as context. We will use a window size of 5 words either side of each word."
      ],
      "metadata": {
        "id": "bUNRQUlJ7i8O"
      },
      "id": "bUNRQUlJ7i8O"
    },
    {
      "cell_type": "code",
      "source": [
        "M_co_occurrence, word2Ind_co_occurrence = compute_co_occurrence_matrix(C_and_P_tokens_sentences, window_size=5)\n",
        "\n",
        "semantic_space = pd.DataFrame(M_co_occurrence, index=word2Ind_co_occurrence.keys(), columns=word2Ind_co_occurrence.keys())"
      ],
      "metadata": {
        "id": "Ih_JNBA47l67"
      },
      "id": "Ih_JNBA47l67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the size of the matrix"
      ],
      "metadata": {
        "id": "iprBqat77mbF"
      },
      "id": "iprBqat77mbF"
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_space.shape"
      ],
      "metadata": {
        "id": "6hwOsjhW7qNU"
      },
      "id": "6hwOsjhW7qNU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at a part of the semantic space like this:"
      ],
      "metadata": {
        "id": "KyxQNelJ7tbz"
      },
      "id": "KyxQNelJ7tbz"
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_space.head(20)"
      ],
      "metadata": {
        "id": "lIoTfTvV7vKd"
      },
      "id": "lIoTfTvV7vKd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And another example part like this:"
      ],
      "metadata": {
        "id": "2Qtos7Jt7v55"
      },
      "id": "2Qtos7Jt7v55"
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_space.iloc[200:220,200:220]"
      ],
      "metadata": {
        "id": "m6g-GgRF70fZ"
      },
      "id": "m6g-GgRF70fZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving our vectors"
      ],
      "metadata": {
        "id": "merJoyqzLkBu"
      },
      "id": "merJoyqzLkBu"
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_space.reset_index(level=0, inplace=True)\n",
        "np.savetxt(r'np.txt', semantic_space.values,fmt='%s')"
      ],
      "metadata": {
        "id": "Ht_q27jnLonw"
      },
      "id": "Ht_q27jnLonw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd4a9307-970d-4616-a331-8562959f7d47"
      },
      "source": [
        "# Using our Vectors"
      ],
      "id": "fd4a9307-970d-4616-a331-8562959f7d47"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "969aeb9c-3268-4fd9-9b17-12d3d5cff87e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from annoy import AnnoyIndex\n",
        "import numpy as np"
      ],
      "id": "969aeb9c-3268-4fd9-9b17-12d3d5cff87e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35656cef-b4d1-49c2-a399-91133a7b8fce"
      },
      "outputs": [],
      "source": [
        "# Function from Rao, D., & McMahan, B. (2019). Natural language processing with PyTorch: build intelligent language applications using deep learning. \" O'Reilly Media, Inc.\".\n",
        "class EmbeddingUtil(object):\n",
        "    \"\"\" A wrapper around pre-trained word vectors and their use \"\"\"\n",
        "    def __init__(self, word_to_index, word_vectors):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            word_to_index (dict): mapping from word to integers\n",
        "            word_vectors (list of numpy arrays)\n",
        "        \"\"\"\n",
        "        self.word_to_index = word_to_index\n",
        "        self.word_vectors = word_vectors\n",
        "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
        "\n",
        "        self.index = AnnoyIndex(len(word_vectors[0]), metric='angular')\n",
        "        print(\"Building Index!\")\n",
        "        for _, i in self.word_to_index.items():\n",
        "            self.index.add_item(i, self.word_vectors[i])\n",
        "        self.index.build(50)\n",
        "        print(\"Finished!\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_embeddings_file(cls, embedding_file):\n",
        "        \"\"\"Instantiate from pre-trained vector file.\n",
        "\n",
        "        Vector file should be of the format:\n",
        "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n",
        "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n",
        "\n",
        "        Args:\n",
        "            embedding_file (str): location of the file\n",
        "        Returns:\n",
        "            instance of PretrainedEmbeddigns\n",
        "        \"\"\"\n",
        "        word_to_index = {}\n",
        "        word_vectors = []\n",
        "\n",
        "        with open(embedding_file) as fp:\n",
        "            for line in fp.readlines():\n",
        "                line = line.split(\" \")\n",
        "                word = line[0]\n",
        "                vec = np.array([float(x) for x in line[1:]])\n",
        "\n",
        "                word_to_index[word] = len(word_to_index)\n",
        "                word_vectors.append(vec)\n",
        "\n",
        "        return cls(word_to_index, word_vectors)\n",
        "\n",
        "    def get_embedding(self, word):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            word (str)\n",
        "        Returns\n",
        "            an embedding (numpy.ndarray)\n",
        "        \"\"\"\n",
        "        return self.word_vectors[self.word_to_index[word]]\n",
        "\n",
        "    def get_closest_to_vector(self, vector, n=1):\n",
        "        \"\"\"Given a vector, return its n nearest neighbors\n",
        "\n",
        "        Args:\n",
        "            vector (np.ndarray): should match the size of the vectors\n",
        "                in the Annoy index\n",
        "            n (int): the number of neighbors to return\n",
        "        Returns:\n",
        "            [str, str, ...]: words that are nearest to the given vector.\n",
        "                The words are not ordered by distance\n",
        "        \"\"\"\n",
        "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
        "        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
        "\n",
        "    def compute_and_print_analogy(self, word1, word2, word3):\n",
        "        \"\"\"Prints the solutions to analogies using word embeddings\n",
        "\n",
        "        Analogies are word1 is to word2 as word3 is to __\n",
        "        This method will print: word1 : word2 :: word3 : word4\n",
        "\n",
        "        Args:\n",
        "            word1 (str)\n",
        "            word2 (str)\n",
        "            word3 (str)\n",
        "        \"\"\"\n",
        "        vec1 = self.get_embedding(word1)\n",
        "        vec2 = self.get_embedding(word2)\n",
        "        vec3 = self.get_embedding(word3)\n",
        "\n",
        "        # now compute the fourth word's embedding!\n",
        "        spatial_relationship = vec2 - vec1\n",
        "        vec4 = vec3 + spatial_relationship\n",
        "\n",
        "        closest_words = self.get_closest_to_vector(vec4, n=4)\n",
        "        existing_words = set([word1, word2, word3])\n",
        "        closest_words = [word for word in closest_words\n",
        "                             if word not in existing_words]\n",
        "\n",
        "        if len(closest_words) == 0:\n",
        "            print(\"Could not find nearest neighbors for the computed vector!\")\n",
        "            return\n",
        "\n",
        "        for word4 in closest_words:\n",
        "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4))"
      ],
      "id": "35656cef-b4d1-49c2-a399-91133a7b8fce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c87a10de-0ac2-4116-864c-7ddd7ec9e197"
      },
      "outputs": [],
      "source": [
        "embeddings = EmbeddingUtil.from_embeddings_file('np.txt')"
      ],
      "id": "c87a10de-0ac2-4116-864c-7ddd7ec9e197"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "438ad7ef-c20e-402d-ade5-9eda1bad6983"
      },
      "outputs": [],
      "source": [
        "vec=embeddings.get_embedding(\"child\")\n",
        "print(vec)"
      ],
      "id": "438ad7ef-c20e-402d-ade5-9eda1bad6983"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6119bba-e949-48ff-ad40-0b826968177a"
      },
      "outputs": [],
      "source": [
        "embeddings.get_closest_to_vector(vec, n=4)"
      ],
      "id": "f6119bba-e949-48ff-ad40-0b826968177a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3fcdc3b-ee92-4181-b901-6fe7ff50cd64"
      },
      "source": [
        "# Pretrained Embeddings\n",
        "Vectors are best when learned from very large text collections. However learning such vectors, particular using neural network methods, is very computationally intensive. As a result most people make use of pretrained embeddings such as those found at\n",
        "\n",
        "https://code.google.com/archive/p/word2vec/\n",
        "\n",
        "or\n",
        "\n",
        "https://nlp.stanford.edu/projects/glove/\n"
      ],
      "id": "d3fcdc3b-ee92-4181-b901-6fe7ff50cd64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baf61f39-face-4645-a849-7f03c840f4bd"
      },
      "outputs": [],
      "source": [
        "embeddings = EmbeddingUtil.from_embeddings_file('glove.6B.100d.txt')"
      ],
      "id": "baf61f39-face-4645-a849-7f03c840f4bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baf9a9d2-eff9-47aa-9e85-aad713a6c19c"
      },
      "outputs": [],
      "source": [
        "vec=embeddings.get_embedding(\"child\")\n",
        "print(vec)"
      ],
      "id": "baf9a9d2-eff9-47aa-9e85-aad713a6c19c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "382d5835-77ed-4fb4-9452-cae913ebbc61"
      },
      "outputs": [],
      "source": [
        "embeddings.get_closest_to_vector(vec, n=4)"
      ],
      "id": "382d5835-77ed-4fb4-9452-cae913ebbc61"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47e0b00f-2595-4402-8492-54e0ee364824"
      },
      "source": [
        "Another semantic property of embeddings is their ability to capture relational meanings. In an important early vector space model of cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model for solving simple analogy problems of the form a is to b as a* is to what?. In such problems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as  grape is to , and must fill in the word vine.\n",
        "\n",
        "In the parallelogram model, the vector from the word apple to the word tree (= apple − tree) is added to the vector for grape (grape); the nearest word to that point is returned.\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "47e0b00f-2595-4402-8492-54e0ee364824"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "065739ee-7ef2-4471-b33e-6d61c41cf7f2"
      },
      "outputs": [],
      "source": [
        "embeddings.compute_and_print_analogy('fly', 'plane', 'sail')"
      ],
      "id": "065739ee-7ef2-4471-b33e-6d61c41cf7f2"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}