{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RByeKCfdaSZ7"
      },
      "source": [
        "# LELA32051 Training a Perceptron\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1U85sASKSul"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/compling23/main/CL_Week_4_Materials/model.pth\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling23/main/CL_Week_4_Materials/nn_tools.py\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling23/main/CL_Week_4_Materials/nn_tools2.py\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling23/main/CL_Week_4_Materials/reviews_with_splits_lite.csv\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling23/main/CL_Week_4_Materials/vectorizer.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqFmkcr0oiYY"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from nn_tools import Vocabulary, ReviewVectorizer, ReviewDataset, ReviewClassifier\n",
        "from nn_tools2 import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYratCznHUbT"
      },
      "source": [
        "### Create Dummy Data for a classifier\n",
        "Imagine a set of one word reviews of products using a vocabulary  with two semantic *dimensions* and two possible labels (negative or positive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PomHIMLiEYtv"
      },
      "outputs": [],
      "source": [
        "WORD1_CENTER = (3, 3)\n",
        "WORD2_CENTER = (3, -2)\n",
        "def get_toy_data(batch_size, w1_center=WORD1_CENTER, w2_center=WORD2_CENTER):\n",
        "      x_data = []\n",
        "      y_targets = np.zeros(batch_size)\n",
        "      for batch_i in range(batch_size):\n",
        "          if np.random.random() > 0.5:\n",
        "              x_data.append(np.random.normal(loc=w1_center))\n",
        "          else:\n",
        "              x_data.append(np.random.normal(loc=w2_center))\n",
        "              y_targets[batch_i] = 1\n",
        "      return torch.tensor(x_data, dtype=torch.float32), torch.tensor(y_targets, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO1jSu46E-5W"
      },
      "source": [
        "### Plot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NaDUzmMEyTt"
      },
      "outputs": [],
      "source": [
        "seed = 1337\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "x_data, y_truth = get_toy_data(batch_size=1000)\n",
        "\n",
        "x_data = x_data.data.numpy()\n",
        "y_truth = y_truth.data.numpy()\n",
        "\n",
        "left_x = []\n",
        "right_x = []\n",
        "left_colors = []\n",
        "right_colors =  []\n",
        "\n",
        "for x_i, y_true_i in zip(x_data, y_truth):\n",
        "    color = 'black'\n",
        "\n",
        "    if y_true_i == 0:\n",
        "        left_x.append(x_i)\n",
        "        left_colors.append(color)\n",
        "\n",
        "    else:\n",
        "        right_x.append(x_i)\n",
        "        right_colors.append(color)\n",
        "\n",
        "left_x = np.stack(left_x)\n",
        "right_x = np.stack(right_x)\n",
        "\n",
        "_, ax = plt.subplots(1, 1, figsize=(10,4))\n",
        "\n",
        "ax.scatter(left_x[:, 0], left_x[:, 1], color=left_colors, marker='*', s=100)\n",
        "ax.scatter(right_x[:, 0], right_x[:, 1], facecolor='white', edgecolor=right_colors, marker='o', s=100)\n",
        "\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sutvjVppNpWJ"
      },
      "source": [
        "### Defining our Perceptron\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQA_P6NyEyYR"
      },
      "outputs": [],
      "source": [
        "class Perceptron(nn.Module):\n",
        "    \"\"\" A Perceptron is one Linear layer \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): size of the input features\n",
        "        \"\"\"\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x_in):\n",
        "        \"\"\"The forward pass of the MLP\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, input_dim)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, 1)\n",
        "        \"\"\"\n",
        "        return torch.sigmoid(self.fc1(x_in))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vt4o2Q-IX8N"
      },
      "source": [
        "We can then train that Perceptron to assign labels to our 1 word reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pSsgJ_1NpWK"
      },
      "outputs": [],
      "source": [
        "lr = 0.01\n",
        "input_dim = 2\n",
        "\n",
        "batch_size = 1000\n",
        "n_epochs = 12\n",
        "n_batches = 5\n",
        "\n",
        "seed = 1337\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "perceptron = Perceptron(input_dim=input_dim)\n",
        "optimizer = optim.Adam(params=perceptron.parameters(), lr=lr)\n",
        "bce_loss = nn.BCELoss()\n",
        "\n",
        "losses = []\n",
        "\n",
        "x_data_static, y_truth_static = get_toy_data(batch_size)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
        "visualize_results(perceptron, x_data_static, y_truth_static, ax=ax, title='Initial Model State')\n",
        "plt.axis('off')\n",
        "\n",
        "change = 1.0\n",
        "last = 10.0\n",
        "epsilon = 1e-3\n",
        "epoch = 0\n",
        "while change > epsilon or epoch < n_epochs or last > 0.3:\n",
        "#for epoch in range(n_epochs):\n",
        "    for _ in range(n_batches):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        x_data, y_target = get_toy_data(batch_size)\n",
        "        y_pred = perceptron(x_data).squeeze()\n",
        "        loss = bce_loss(y_pred, y_target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        loss_value = loss.item()\n",
        "        losses.append(loss_value)\n",
        "\n",
        "        change = abs(last - loss_value)\n",
        "        last = loss_value\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
        "    visualize_results(perceptron, x_data_static, y_truth_static, ax=ax, epoch=epoch,\n",
        "                      title=f\"{loss_value}; {change}\")\n",
        "    plt.axis('off')\n",
        "    epoch += 1\n",
        "    #plt.savefig('epoch{}_toylearning.png'.format(epoch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Week_4_Seminar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "156px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": "5",
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}